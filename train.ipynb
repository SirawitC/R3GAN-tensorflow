{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ae2585d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import os\n",
    "from training.trainer import AdversarialTrainer\n",
    "from models.generator import Generator\n",
    "from models.discriminator import Discriminator\n",
    "from configs.config import *\n",
    "from tensorflow.keras.datasets import cifar10\n",
    "from utils.logger import create_logger"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "745706fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(\"GPU(s) detected:\")\n",
    "    for gpu in gpus:\n",
    "        print(gpu)\n",
    "else:\n",
    "    print(\"No GPU detected by TensorFlow.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afd3d94c",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(trainer, real_images, conditions, noise_dim):\n",
    "    batch_size = tf.shape(real_images)[0]\n",
    "    noise = tf.random.normal([batch_size, noise_dim])\n",
    "    \n",
    "    # Train Discriminator\n",

    "    d_start = time.time()\n",
    "    disc_loss, r1_pen, r2_pen = trainer.train_discriminator(noise, real_images, conditions)\n",
    "    d_time = time.time() - d_start\n",
    "\n",
    "    # Train Generator\n",
    "    g_start = time.time()\n",
    "    gen_loss = trainer.train_generator(noise, real_images, conditions)\n",
    "    g_time = time.time() - g_start\n",
    "\n",
    "    return gen_loss, disc_loss, r1_pen, r2_pen, d_time, g_time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "093bb832",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models\n",
    "generator = Generator(NOISE_DIMENSION_G, WIDTH_PER_STAGE_G, CARDINALITY_PER_STAGE_G, \n",
    "                     BLOCKS_PER_STAGE_G, EXPANSION_FACTOR, CONDITION_DIM, CONDITION_EMBEDDING_DIM_G)\n",
    "discriminator = Discriminator(WIDTH_PER_STAGE_D, CARDINALITY_PER_STAGE_D, BLOCKS_PER_STAGE_D, \n",
    "                             EXPANSION_FACTOR, CONDITION_DIM, CONDITION_EMBEDDING_DIM_D)\n",
    "\n",
    "# Create optimizers\n",
    "total_steps = int((NUM_EPOCHS * IMAGES_PER_EPOCH) / BATCH_SIZE)\n",
    "\n",
    "# Cosine decay goes from initial_lr -> alpha * initial_lr\n",
    "# To match final_lr exactly, set alpha = final_lr / initial_lr\n",
    "alpha_g = FIN_LR_G / INIT_LR_G\n",
    "alpha_d = FIN_LR_D / INIT_LR_D\n",
    "\n",
    "alpha_beta_2 = INIT_BETA_2 / FIN_BETA_2\n",
    "\n",
    "g_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=INIT_LR_G,\n",
    "    decay_steps=total_steps,\n",
    "    alpha=alpha_g\n",
    ")\n",
    "\n",
    "d_lr_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=INIT_LR_D,\n",
    "    decay_steps=total_steps,\n",
    "    alpha=alpha_d\n",
    ")\n",
    "\n",
    "beta_2_scheduler = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=INIT_BETA_2,\n",
    "    decay_steps=total_steps,\n",
    "    alpha=alpha_d\n",
    ")\n",
    "\n",
    "g_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=g_lr_schedule, \n",
    "    beta_1=BETA_1, \n",
    "    beta_2=beta_2_scheduler,\n",
    "    epsilon=1e-8\n",
    ")\n",
    "\n",
    "d_optimizer = tf.keras.optimizers.Adam(\n",
    "    learning_rate=d_lr_schedule, \n",
    "    beta_1=BETA_1, \n",
    "    beta_2=beta_2_scheduler,\n",
    "    epsilon=1e-8\n",
    ")\n",
    "\n",
    "# Cosine decay for gamma (R1/R2 penalty strength)\n",
    "gamma_schedule = tf.keras.optimizers.schedules.CosineDecay(\n",
    "    initial_learning_rate=INIT_GAMMA,                # CIFAR-10 starting Î³\n",
    "    decay_steps=total_steps,\n",
    "    alpha=FIN_GAMMA / INIT_GAMMA                       \n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = AdversarialTrainer(generator, discriminator, g_optimizer, d_optimizer, gamma_schedule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4b9a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup logging\n",
    "run_dir = f\"runs/gan_training_{int(time.time())}\"\n",
    "os.makedirs(run_dir, exist_ok=True)\n",
    "\n",
    "# Initialize logger\n",
    "logger = create_logger(run_dir)\n",
    "\n",
    "# Training parameters\n",
    "total_kimg = NUM_EPOCHS * IMAGES_PER_EPOCH // 1000  # Total images in thousands\n",
    "kimg_per_tick = 50  # Log every 50k images\n",
    "image_snapshot_ticks = 10  # Save images every 10 ticks\n",
    "network_snapshot_ticks = 20  # Save model every 20 ticks\n",
    "\n",
    "# Training state\n",
    "start_time = time.time()\n",
    "cur_nimg = 0\n",
    "cur_tick = 0\n",
    "tick_start_nimg = 0\n",
    "tick_start_time = start_time\n",
    "maintenance_time = 0.0\n",
    "\n",
    "# Fixed noise for consistent image generation\n",
    "num_samples = 16\n",
    "fixed_noise = tf.random.normal([num_samples, NOISE_DIMENSION_G])\n",
    "# Create fixed conditions (you may need to adjust this based on your condition format)\n",
    "fixed_conditions = tf.eye(num_samples, CONDITION_DIM)  # Modify as needed\n",
    "\n",
    "print(f\"Starting training for {NUM_EPOCHS} epochs ({total_kimg} kimg total)\")\n",
    "print(f\"Logging to: {run_dir}\")\n",
    "print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e75c006",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load CIFAR-10\n",
    "(x_train, y_train), (_, _) = cifar10.load_data()\n",
    "\n",
    "# Normalize to [-1, 1]\n",
    "x_train = (x_train.astype(\"float32\") / 127.5) - 1.0\n",
    "\n",
    "# One-hot encode the labels\n",
    "y_train = tf.keras.utils.to_categorical(y_train, CONDITION_DIM)\n",
    "\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
    "\n",
    "train_dataset = train_dataset.shuffle(50000).batch(BATCH_SIZE).prefetch(tf.data.AUTOTUNE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf49f439",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:    \n",
    "    for epoch in range(NUM_EPOCHS):\n",
    "        epoch_start_time = time.time()\n",
    "        \n",
    "        # Phase timing\n",
    "        d_times = []\n",
    "        g_times = []\n",
    "        \n",
    "        batch_count = 0\n",
    "        epoch_losses = {'gen_loss': [], 'disc_loss': [], 'r1_pen': [], 'r2_pen': []}\n",
    "\n",
    "        for real_images, conditions in train_dataset:\n",
    "            batch_start_time = time.time()\n",
    "            \n",
    "            gen_loss, disc_loss, r1, r2, d_time, g_time = train_step(trainer, real_images, conditions, NOISE_DIMENSION_G)\n",
    "            \n",
    "            d_times.append(d_time)\n",
    "            g_times.append(g_time)\n",
    "            \n",
    "            # Store losses for epoch averaging\n",
    "            epoch_losses['gen_loss'].append(float(gen_loss))\n",
    "            epoch_losses['disc_loss'].append(float(disc_loss))\n",
    "            epoch_losses['r1_pen'].append(float(r1[0]))\n",
    "            epoch_losses['r2_pen'].append(float(r2[0]))\n",
    "            \n",
    "            # Update image count\n",
    "            batch_size = tf.shape(real_images)[0]\n",
    "            cur_nimg += int(batch_size)\n",
    "            batch_count += 1\n",
    "            \n",
    "            # Log every batch (optional, might be too frequent)\n",
    "            if batch_count % 10 == 0:  # Log every 10 batches\n",
    "                logger.log_losses(\n",
    "                    gen_loss=gen_loss,\n",
    "                    disc_loss=disc_loss,\n",
    "                    r1_penalty=r1[0],\n",
    "                    r2_penalty=r2[0]\n",
    "                )\n",
    "            if batch_count % 500 == 0:\n",
    "                print(f\"Epoch {epoch+1}/{NUM_EPOCHS} (batch {batch_count})| gen_loss: {float(gen_loss)}, disc_loss: {float(disc_loss)}, r1: {float(r1[0])}, r2: {float(r2[0])}\")            \n",
    "        \n",
    "        # End of epoch logging\n",
    "        epoch_end_time = time.time()\n",
    "        epoch_time = epoch_end_time - epoch_start_time\n",
    "        \n",
    "        # Calculate average losses for the epoch\n",
    "        avg_losses = {k: sum(v) / len(v) for k, v in epoch_losses.items()}\n",
    "        \n",
    "        # Calculate average phase times\n",
    "        avg_d_time = sum(d_times) / len(d_times) if d_times else 0\n",
    "        avg_g_time = sum(g_times) / len(g_times) if g_times else 0\n",
    "        \n",
    "        # Check if we need to log (tick-based logging)\n",
    "        if cur_nimg >= tick_start_nimg + kimg_per_tick * 1000:\n",
    "            maintenance_start = time.time()\n",
    "            \n",
    "            # Log progress\n",
    "            logger.log_progress(\n",
    "                cur_tick=cur_tick,\n",
    "                cur_nimg=cur_nimg,\n",
    "                total_kimg=total_kimg,\n",
    "                tick_start_time=tick_start_time,\n",
    "                tick_start_nimg=tick_start_nimg,\n",
    "                start_time=start_time,\n",
    "                maintenance_time=maintenance_time,\n",
    "                cur_lr=float(g_optimizer.learning_rate),\n",
    "                cur_ema_nimg=cur_nimg * 0.5,  # Approximate EMA parameter\n",
    "                cur_beta2=float(g_optimizer.beta_2),\n",
    "                cur_gamma=10.0,  # R1/R2 penalty weight\n",
    "                augment_p=0.0,  # Set augmentation probability if used\n",
    "                phase_times={'D': avg_d_time, 'G': avg_g_time}\n",
    "            )\n",
    "            \n",
    "            # Log average losses\n",
    "            logger.log_losses(**avg_losses)\n",
    "            \n",
    "            # Generate and log sample images\n",
    "            if cur_tick % image_snapshot_ticks == 0:\n",
    "                try:\n",
    "                    generated_images = trainer.generator_ema(fixed_noise, fixed_conditions, training=False)\n",
    "                    logger.log_images(generated_images, step=cur_nimg//1000, tag=\"generated_samples\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not log images: {e}\")\n",
    "            \n",
    "            # Save model checkpoint\n",
    "            if cur_tick % network_snapshot_ticks == 0:\n",
    "                try:\n",
    "                    checkpoint_dir = os.path.join(run_dir, f\"checkpoint_{cur_nimg//1000:06d}\")\n",
    "                    os.makedirs(checkpoint_dir, exist_ok=True)\n",
    "                    trainer.generator_ema.save_weights(os.path.join(checkpoint_dir, \"generator_ema\"))\n",
    "                    discriminator.save_weights(os.path.join(checkpoint_dir, \"discriminator\"))\n",
    "                    print(f\"Saved checkpoint at {checkpoint_dir}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not save checkpoint: {e}\")\n",
    "            \n",
    "            # Update logs\n",
    "            logger.update_logs(cur_nimg, start_time)\n",
    "            \n",
    "            # Update tick state\n",
    "            cur_tick += 1\n",
    "            tick_start_nimg = cur_nimg\n",
    "            tick_start_time = time.time()\n",
    "            maintenance_time = tick_start_time - maintenance_start\n",
    "        \n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch {epoch+1}/{NUM_EPOCHS}: \"\n",
    "              f\"G Loss = {avg_losses['gen_loss']:.4f}, \"\n",
    "              f\"D Loss = {avg_losses['disc_loss']:.4f}, \"\n",
    "              f\"R1 = {avg_losses['r1_pen']:.4f}, \"\n",
    "              f\"R2 = {avg_losses['r2_pen']:.4f}, \"\n",
    "              f\"Time = {epoch_time:.1f}s\")\n",
    "        print(\"=\"*20)\n",
    "\n",
    "    print(\"\\nTraining completed!\")\n",
    "\n",
    "except KeyboardInterrupt:\n",
    "    print(\"\\nTraining interrupted by user\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\nTraining failed with error: {e}\")\n",
    "    raise\n",
    "\n",
    "finally:\n",
    "    # Clean up\n",
    "    logger.close()\n",
    "    print(f\"Logs saved to: {run_dir}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf215_gpu",
   "language": "python",
   "name": "tf215_gpu"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
